{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set: Tensorflow\n",
    "> Tangqi Feng\n",
    "\n",
    "These problems relate to the Python package [Tensorflow](https://www.tensorflow.org/).\n",
    "We will again use the famous [iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and convert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.1  3.5  1.4  0.2  1. ]\n",
      " [ 4.9  3.   1.4  0.2  1. ]\n",
      " [ 4.7  3.2  1.3  0.2  1. ]\n",
      " [ 4.6  3.1  1.5  0.2  1. ]\n",
      " [ 5.   3.6  1.4  0.2  1. ]\n",
      " [ 5.4  3.9  1.7  0.4  1. ]\n",
      " [ 4.6  3.4  1.4  0.3  1. ]\n",
      " [ 5.   3.4  1.5  0.2  1. ]\n",
      " [ 4.4  2.9  1.4  0.2  1. ]\n",
      " [ 4.9  3.1  1.5  0.1  1. ]\n",
      " [ 5.4  3.7  1.5  0.2  1. ]\n",
      " [ 4.8  3.4  1.6  0.2  1. ]\n",
      " [ 4.8  3.   1.4  0.1  1. ]\n",
      " [ 4.3  3.   1.1  0.1  1. ]\n",
      " [ 5.8  4.   1.2  0.2  1. ]\n",
      " [ 5.7  4.4  1.5  0.4  1. ]\n",
      " [ 5.4  3.9  1.3  0.4  1. ]\n",
      " [ 5.1  3.5  1.4  0.3  1. ]\n",
      " [ 5.7  3.8  1.7  0.3  1. ]\n",
      " [ 5.1  3.8  1.5  0.3  1. ]\n",
      " [ 5.4  3.4  1.7  0.2  1. ]\n",
      " [ 5.1  3.7  1.5  0.4  1. ]\n",
      " [ 4.6  3.6  1.   0.2  1. ]\n",
      " [ 5.1  3.3  1.7  0.5  1. ]\n",
      " [ 4.8  3.4  1.9  0.2  1. ]\n",
      " [ 5.   3.   1.6  0.2  1. ]\n",
      " [ 5.   3.4  1.6  0.4  1. ]\n",
      " [ 5.2  3.5  1.5  0.2  1. ]\n",
      " [ 5.2  3.4  1.4  0.2  1. ]\n",
      " [ 4.7  3.2  1.6  0.2  1. ]\n",
      " [ 4.8  3.1  1.6  0.2  1. ]\n",
      " [ 5.4  3.4  1.5  0.4  1. ]\n",
      " [ 5.2  4.1  1.5  0.1  1. ]\n",
      " [ 5.5  4.2  1.4  0.2  1. ]\n",
      " [ 4.9  3.1  1.5  0.1  1. ]\n",
      " [ 5.   3.2  1.2  0.2  1. ]\n",
      " [ 5.5  3.5  1.3  0.2  1. ]\n",
      " [ 4.9  3.1  1.5  0.1  1. ]\n",
      " [ 4.4  3.   1.3  0.2  1. ]\n",
      " [ 5.1  3.4  1.5  0.2  1. ]\n",
      " [ 5.   3.5  1.3  0.3  1. ]\n",
      " [ 4.5  2.3  1.3  0.3  1. ]\n",
      " [ 4.4  3.2  1.3  0.2  1. ]\n",
      " [ 5.   3.5  1.6  0.6  1. ]\n",
      " [ 5.1  3.8  1.9  0.4  1. ]\n",
      " [ 4.8  3.   1.4  0.3  1. ]\n",
      " [ 5.1  3.8  1.6  0.2  1. ]\n",
      " [ 4.6  3.2  1.4  0.2  1. ]\n",
      " [ 5.3  3.7  1.5  0.2  1. ]\n",
      " [ 5.   3.3  1.4  0.2  1. ]\n",
      " [ 7.   3.2  4.7  1.4  2. ]\n",
      " [ 6.4  3.2  4.5  1.5  2. ]\n",
      " [ 6.9  3.1  4.9  1.5  2. ]\n",
      " [ 5.5  2.3  4.   1.3  2. ]\n",
      " [ 6.5  2.8  4.6  1.5  2. ]\n",
      " [ 5.7  2.8  4.5  1.3  2. ]\n",
      " [ 6.3  3.3  4.7  1.6  2. ]\n",
      " [ 4.9  2.4  3.3  1.   2. ]\n",
      " [ 6.6  2.9  4.6  1.3  2. ]\n",
      " [ 5.2  2.7  3.9  1.4  2. ]\n",
      " [ 5.   2.   3.5  1.   2. ]\n",
      " [ 5.9  3.   4.2  1.5  2. ]\n",
      " [ 6.   2.2  4.   1.   2. ]\n",
      " [ 6.1  2.9  4.7  1.4  2. ]\n",
      " [ 5.6  2.9  3.6  1.3  2. ]\n",
      " [ 6.7  3.1  4.4  1.4  2. ]\n",
      " [ 5.6  3.   4.5  1.5  2. ]\n",
      " [ 5.8  2.7  4.1  1.   2. ]\n",
      " [ 6.2  2.2  4.5  1.5  2. ]\n",
      " [ 5.6  2.5  3.9  1.1  2. ]\n",
      " [ 5.9  3.2  4.8  1.8  2. ]\n",
      " [ 6.1  2.8  4.   1.3  2. ]\n",
      " [ 6.3  2.5  4.9  1.5  2. ]\n",
      " [ 6.1  2.8  4.7  1.2  2. ]\n",
      " [ 6.4  2.9  4.3  1.3  2. ]\n",
      " [ 6.6  3.   4.4  1.4  2. ]\n",
      " [ 6.8  2.8  4.8  1.4  2. ]\n",
      " [ 6.7  3.   5.   1.7  2. ]\n",
      " [ 6.   2.9  4.5  1.5  2. ]\n",
      " [ 5.7  2.6  3.5  1.   2. ]\n",
      " [ 5.5  2.4  3.8  1.1  2. ]\n",
      " [ 5.5  2.4  3.7  1.   2. ]\n",
      " [ 5.8  2.7  3.9  1.2  2. ]\n",
      " [ 6.   2.7  5.1  1.6  2. ]\n",
      " [ 5.4  3.   4.5  1.5  2. ]\n",
      " [ 6.   3.4  4.5  1.6  2. ]\n",
      " [ 6.7  3.1  4.7  1.5  2. ]\n",
      " [ 6.3  2.3  4.4  1.3  2. ]\n",
      " [ 5.6  3.   4.1  1.3  2. ]\n",
      " [ 5.5  2.5  4.   1.3  2. ]\n",
      " [ 5.5  2.6  4.4  1.2  2. ]\n",
      " [ 6.1  3.   4.6  1.4  2. ]\n",
      " [ 5.8  2.6  4.   1.2  2. ]\n",
      " [ 5.   2.3  3.3  1.   2. ]\n",
      " [ 5.6  2.7  4.2  1.3  2. ]\n",
      " [ 5.7  3.   4.2  1.2  2. ]\n",
      " [ 5.7  2.9  4.2  1.3  2. ]\n",
      " [ 6.2  2.9  4.3  1.3  2. ]\n",
      " [ 5.1  2.5  3.   1.1  2. ]\n",
      " [ 5.7  2.8  4.1  1.3  2. ]\n",
      " [ 6.3  3.3  6.   2.5  3. ]\n",
      " [ 5.8  2.7  5.1  1.9  3. ]\n",
      " [ 7.1  3.   5.9  2.1  3. ]\n",
      " [ 6.3  2.9  5.6  1.8  3. ]\n",
      " [ 6.5  3.   5.8  2.2  3. ]\n",
      " [ 7.6  3.   6.6  2.1  3. ]\n",
      " [ 4.9  2.5  4.5  1.7  3. ]\n",
      " [ 7.3  2.9  6.3  1.8  3. ]\n",
      " [ 6.7  2.5  5.8  1.8  3. ]\n",
      " [ 7.2  3.6  6.1  2.5  3. ]\n",
      " [ 6.5  3.2  5.1  2.   3. ]\n",
      " [ 6.4  2.7  5.3  1.9  3. ]\n",
      " [ 6.8  3.   5.5  2.1  3. ]\n",
      " [ 5.7  2.5  5.   2.   3. ]\n",
      " [ 5.8  2.8  5.1  2.4  3. ]\n",
      " [ 6.4  3.2  5.3  2.3  3. ]\n",
      " [ 6.5  3.   5.5  1.8  3. ]\n",
      " [ 7.7  3.8  6.7  2.2  3. ]\n",
      " [ 7.7  2.6  6.9  2.3  3. ]\n",
      " [ 6.   2.2  5.   1.5  3. ]\n",
      " [ 6.9  3.2  5.7  2.3  3. ]\n",
      " [ 5.6  2.8  4.9  2.   3. ]\n",
      " [ 7.7  2.8  6.7  2.   3. ]\n",
      " [ 6.3  2.7  4.9  1.8  3. ]\n",
      " [ 6.7  3.3  5.7  2.1  3. ]\n",
      " [ 7.2  3.2  6.   1.8  3. ]\n",
      " [ 6.2  2.8  4.8  1.8  3. ]\n",
      " [ 6.1  3.   4.9  1.8  3. ]\n",
      " [ 6.4  2.8  5.6  2.1  3. ]\n",
      " [ 7.2  3.   5.8  1.6  3. ]\n",
      " [ 7.4  2.8  6.1  1.9  3. ]\n",
      " [ 7.9  3.8  6.4  2.   3. ]\n",
      " [ 6.4  2.8  5.6  2.2  3. ]\n",
      " [ 6.3  2.8  5.1  1.5  3. ]\n",
      " [ 6.1  2.6  5.6  1.4  3. ]\n",
      " [ 7.7  3.   6.1  2.3  3. ]\n",
      " [ 6.3  3.4  5.6  2.4  3. ]\n",
      " [ 6.4  3.1  5.5  1.8  3. ]\n",
      " [ 6.   3.   4.8  1.8  3. ]\n",
      " [ 6.9  3.1  5.4  2.1  3. ]\n",
      " [ 6.7  3.1  5.6  2.4  3. ]\n",
      " [ 6.9  3.1  5.1  2.3  3. ]\n",
      " [ 5.8  2.7  5.1  1.9  3. ]\n",
      " [ 6.8  3.2  5.9  2.3  3. ]\n",
      " [ 6.7  3.3  5.7  2.5  3. ]\n",
      " [ 6.7  3.   5.2  2.3  3. ]\n",
      " [ 6.3  2.5  5.   1.9  3. ]\n",
      " [ 6.5  3.   5.2  2.   3. ]\n",
      " [ 6.2  3.4  5.4  2.3  3. ]\n",
      " [ 5.9  3.   5.1  1.8  3. ]]\n"
     ]
    }
   ],
   "source": [
    "# inport numpy\n",
    "import numpy as np\n",
    "# load Iris data set\n",
    "OriginalData = np.loadtxt(\"iris.csv\",str, delimiter=\",\", skiprows=1, unpack=True)\n",
    "Iris = OriginalData.transpose()\n",
    "# change 4th column data (species name) to a number\n",
    "#  setosa     ->   1\n",
    "#  virginica  ->   2\n",
    "#  versicolor ->   3\n",
    "Iris[Iris[:,4] == 'setosa',4] = '1';\n",
    "Iris[Iris[:,4] == 'versicolor',4] = '2';\n",
    "Iris[Iris[:,4] == 'virginica',4] = '3';\n",
    "# convert array(Iris) from type(str) to float\n",
    "Iris = np.array(Iris).astype(np.float)\n",
    "print(Iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Use Tensorflow to create model\n",
    "Use Tensorflow to create a model to predict the species of Iris from a flower's sepal width, sepal length, petal width, and petal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow\n",
    "import tensorflow as tf\n",
    "# a set of data contains: sepal_length, sepal_width, petal_length, petal_width and species\n",
    "# create a model\n",
    "x = tf.placeholder(tf.float32,[None,4])  #input_data  (sepal_length, sepal_width, petal_length, petal_width)\n",
    "y = tf.placeholder(tf.float32,[None,1])  #output_data (species)\n",
    "# tf.truncated_normal method from:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/truncated_normal\n",
    "W = tf.Variable(tf.truncated_normal([4,1],stddev=0.1))  # Weight ([4,1]: 4 input and 1 output)\n",
    "b = tf.Variable(tf.zeros([1]) + 1)                      # bias\n",
    "prediction = tf.nn.softmax(tf.matmul(x,W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split the data into training and testing\n",
    "Split the data set into a training set and a testing set.\n",
    "You should investigate the best way to do this, and list any online references used in your notebook.\n",
    "If you wish to, you can write some code to randomly separate the data on the fly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5)\n",
      "(50, 5)\n"
     ]
    }
   ],
   "source": [
    "# Randomly split the data into training and testing\n",
    "# Adapt from : https://stackoverflow.com/questions/17412439/how-to-split-data-into-trainset-and-testset-randomly\n",
    "np.random.shuffle(Iris)\n",
    "# define 100 set of data for training, and 50 for test\n",
    "training, test = Iris[:100], Iris[100:] \n",
    "print(training.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model\n",
    "Use the training set to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get input data_set and output data_set\n",
    "train_in = training[:,:4]    #(sepal_length, sepal_width, petal_length, petal_width)\n",
    "train_out = training[:,4:]   #(species)\n",
    "# use cross_entropy method: tf.nn.softmax_cross_entropy_with_logits method\n",
    "# https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))\n",
    "# use GradientDescentOptimizer to train\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "# initial glabal variables\n",
    "init = tf.global_variables_initializer()\n",
    "# get number of sets of training data\n",
    "size = training.size\n",
    "\n",
    "# create loop to train\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for eposh in range(10):    # times for training all training)set\n",
    "        sess.run(train_step,{x:train_in, y:train_out})\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test the model\n",
    "Use the testing set to test your model, clearly calculating and displaying the error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
