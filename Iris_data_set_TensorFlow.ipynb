{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set: Tensorflow\n",
    "> Tangqi Feng\n",
    "\n",
    "These problems relate to the Python package [Tensorflow](https://www.tensorflow.org/).\n",
    "We will again use the famous [iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and convert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5.1' '3.5' '1.4' '0.2' 'setosa']\n",
      "['4.9' '3' '1.4' '0.2' 'setosa']\n",
      "['4.7' '3.2' '1.3' '0.2' 'setosa']\n",
      "['4.6' '3.1' '1.5' '0.2' 'setosa']\n",
      "['5' '3.6' '1.4' '0.2' 'setosa']\n"
     ]
    }
   ],
   "source": [
    "# adapt from https://github.com/antonrufino/TensorFlow-IrisNN/blob/master/iris_nn.py\n",
    "# inport numpy\n",
    "import numpy as np\n",
    "# load Iris data set as str,because the 'species' column's type is str\n",
    "OriginalData = np.loadtxt(\"iris.csv\",str, delimiter=\",\", skiprows=1, unpack=True)\n",
    "iris = OriginalData.transpose()\n",
    "\n",
    "# the data stored like this:\n",
    "for i in range(5):\n",
    "    print(iris[i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split the data into training and testing\n",
    "Split the data set into a training set and a testing set.\n",
    "You should investigate the best way to do this, and list any online references used in your notebook.\n",
    "If you wish to, you can write some code to randomly separate the data on the fly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "species: virginica --> 2 --> [ 0.  0.  1.]\n",
      "species: setosa --> 0 --> [ 1.  0.  0.]\n",
      "species: versicolor --> 1 --> [ 0.  1.  0.]\n",
      "species: virginica --> 2 --> [ 0.  0.  1.]\n",
      "species: versicolor --> 1 --> [ 0.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Randomly split the data into training and testing\n",
    "# Adapt from : https://stackoverflow.com/questions/17412439/how-to-split-data-into-trainset-and-testset-randomly\n",
    "np.random.shuffle(iris)\n",
    "# define 100 set of data for training, and 50 for test\n",
    "train, test = iris[:100], iris[100:]\n",
    "# get input data (sepal_length, sepal_width, petal_length, petal_width)\n",
    "in_train = train[:,:4]\n",
    "in_test = test[:,:4]\n",
    "# get output data\n",
    "out_train = train[:,4]\n",
    "out_test = test[:,4]\n",
    "# convert inputs data from type(str) to float\n",
    "in_train = np.array(in_train).astype(np.float)\n",
    "in_test = np.array(in_test).astype(np.float)\n",
    "# get a list of output categories as 'one-hot vectors'\n",
    "# a one-hot vector is a vector which is 0 in most dimensions, and 1 in a single dimension\n",
    "# in this case,convert like this:\n",
    "#    setosa      ->[1,0,0]\n",
    "#    virginica   ->[0,1,0]\n",
    "#    versicolor  ->[0,0,1]\n",
    "# np.unique --> return a unique list in array, \n",
    "# 'return_inverse=True' will return the index number of such unique element\n",
    "# more details: https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.unique.html\n",
    "uniq_test, ids_test = np.unique(out_test, return_inverse=True)\n",
    "uniq_train, ids_train = np.unique(out_train, return_inverse=True)\n",
    "# np_utils.to_categorical --> Converts a class vector (integers) to binary class matrix.\n",
    "# more details: https://keras.io/utils/\n",
    "from keras.utils import np_utils\n",
    "onehot_train = np_utils.to_categorical(ids_train, len(uniq_train))\n",
    "onehot_test = np_utils.to_categorical(ids_test, len(uniq_test))\n",
    "# after np.unique & np_utils.to_categorical two operation\n",
    "# the data like this: \n",
    "for i in range(5):\n",
    "    print('species: '+(str)(out_test[i])+' --> '+(str)(ids_test[i])\\\n",
    "          +' --> '+(str)(onehot_test[i,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use Tensorflow to create model\n",
    "Use Tensorflow to create a model to predict the species of Iris from a flower's sepal width, sepal length, petal width, and petal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a set of data contains: sepal_length, sepal_width, petal_length, petal_width and species\n",
    "# import tensorflow\n",
    "import tensorflow as tf\n",
    "# create a model:\n",
    "# input layer\n",
    "# input_data [None,4] means:None: do not deside the number of set of input numbers\n",
    "#                           4: (sepal_length, sepal_width, petal_length, petal_width)\n",
    "x = tf.placeholder(tf.float32,[None,4])\n",
    "# output_data (species) means:None: do not deside the number of set of input numbers\n",
    "#                           3: species:(setosa,versicolor,virginica)\n",
    "y = tf.placeholder(tf.float32,[None,3])\n",
    "# output layer\n",
    "# tf.truncated_normal --> outputs random values from a truncated normal distribution\n",
    "# 'stddev' is the standard deviation of the truncated normal distribution.\n",
    "# method details: https://www.tensorflow.org/api_docs/python/tf/truncated_normal\n",
    "# here, designed a [4,3] matrix for Weight(W), simply means 4 data inputs and 3 outputs\n",
    "W = tf.Variable(tf.truncated_normal([4,3],stddev=0.1))\n",
    "# set bias for output value, because output 3 values, so the size of bias is [3]\n",
    "b = tf.Variable(tf.zeros([3]) + 1)\n",
    "# tf.matmul --> a method multiplies matrix\n",
    "# tf.nn.softmax --> softmax gives us a list of values between 0 and 1 that add up to 1\n",
    "prediction = tf.nn.softmax(tf.matmul(x,W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model\n",
    "Use the training set to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# here, use cross_entropy and softmax method:\n",
    "# cross_entropy function --> as the loss function\n",
    "# softmax is the thing to do, because softmax gives us a list of values between 0 and 1 that add up to 1\n",
    "# tf.nn.softmax_cross_entropy_with_logits method combines cross_entropy and softmax\n",
    "# more details: https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))\n",
    "# use GradientDescent optimizer to train\n",
    "train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "# initial glabal variables\n",
    "init = tf.global_variables_initializer()\n",
    "# create loop to train\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for eposh in range(100):    # times for training all training)set\n",
    "        sess.run(train_step,{x:in_train, y:onehot_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test the model\n",
    "Use the testing set to test your model, clearly calculating and displaying the error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0,Testing Accuracy 0.38,Testing Error Rate 0.620000004768\n",
      "Iter 10,Testing Accuracy 0.66,Testing Error Rate 0.339999973774\n",
      "Iter 20,Testing Accuracy 0.66,Testing Error Rate 0.339999973774\n",
      "Iter 30,Testing Accuracy 0.66,Testing Error Rate 0.339999973774\n",
      "Iter 40,Testing Accuracy 0.72,Testing Error Rate 0.27999997139\n",
      "Iter 50,Testing Accuracy 0.84,Testing Error Rate 0.160000026226\n",
      "Iter 60,Testing Accuracy 0.88,Testing Error Rate 0.120000004768\n",
      "Iter 70,Testing Accuracy 0.9,Testing Error Rate 0.100000023842\n",
      "Iter 80,Testing Accuracy 0.92,Testing Error Rate 0.0799999833107\n",
      "Iter 90,Testing Accuracy 0.94,Testing Error Rate 0.0600000023842\n",
      "Iter 100,Testing Accuracy 0.98,Testing Error Rate 0.0199999809265\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1)) # correct return true, otherwise return false\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # true->1.0   false->0\n",
    "# create loop to test\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for eposh in range(101):    # times for training all training)set\n",
    "        sess.run(train_step,{x:in_train, y:onehot_train})                  #train               \n",
    "        acc = sess.run(accuracy,{x:in_test, y:onehot_test})                #test\n",
    "        if(eposh%10==0):\n",
    "            print(\"Iter \" + str(eposh) + \",Testing Accuracy \" + str(acc) + \",Testing Error Rate \" + str(1-acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "## here we get 98% accuracy\n",
    "## change some stuff, will get different result\n",
    "## * initial value, here we use 'tf.truncated_normal' function for Weight and 'tf.zeros' for bias\n",
    "## * loss calculate method, here we use 'cross_entropy' function\n",
    "## * activation method, here we use 'softmax' function\n",
    "## * optimizer, here we use GradientDescent optimizer, learning_rate (0.2)\n",
    "## * training, testing data set, here we have 100 training data set and 50 test data set\n",
    "## * times for training, here we trains 101 times\n",
    "## * neural network layer, here we just use a input layer (4 nodes) and a output layer (3 nodes). It is possible to add some middle layer.\n",
    "    for example, input layer (4 nodes) -> middle layer (X nodes) -> middle layer (Y nodes) ........ ->output layer (3 nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
